min_power_cons : 10
max_power_cons : 10
sigma_1 : 0.1
sigma_2 : 1
max_mass_points : 1
epsilon : 0.005 #convergence criteria
lr : [0.0001] # learning rate
max_iter : 50000 # maximum number of iterations
delta_y : 0.1
cons_type : 0 # 0:peak power, 1:average power, 2:first moment
max_k_nochange : 5 # When to stop if the k change does not improve
n_snr : 1 # number of SNRs
nonlinearity : 3 # 0:linear, 1:nonlinear C1 (sgn(X)sqrt(abs(X))), 2: nonlinear C2 (sgn(X)abs(X)^{0.9}), 3:nonlinear tanh(X), 4:nonlinear x4: x/(1 + x^4)^1/4 # 5: nonlinear clip
clipping_limit : 5 # the limit for the clipping if nonlinear is clip (5)
output_dir : 'Results-short' # file name to save the results
tanh_factor : 1 # k when the nonlinearity is tanh(x/k)
regime : 1 # 1: first regime phi(X)+N , 2: second regime phi(X+N), 3: third regime phi(X+Z_1)+Z_2
stop_sd : 4 # the length of the sigma, alphabet_x, pdf_x, powerl is taken as stop_sd*sigma extra while calculating the alphabet
gd_active : True
gd_alphabet_active : False # the gradient descent is active for the alphabet
mass_points : [65] # the mass points for the gradient descent over the alphabet
gd_initial_ba : False # It has to be peak power constraint
bound_active: True
ba_active : False # Blahut Arimoto - only works with the peak power constraint
n_samples: 5000000
time_division_active: True # When this is active, I have only one SNR is active - forced in the code
n_time_division: 10
power_change_active: False # R1 and R2 boosts their power according to the time allocated to them to use the same power for the total duration